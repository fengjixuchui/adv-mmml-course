- date: 1/20
  title: >
    Week 1: <strong>Course introduction</strong> <a href="lecture1-Introduction.pdf">[slides]</a>
  slides:
  topics:
    - Course syllabus and requirements
    - Multimodal principles&#58; heterogeneity, connections, and interactions
    - Multimodal technical challenges
  readings:
    - <a href="https://arxiv.org/abs/2209.03430">Foundations and Trends in Multimodal Machine Learning&#58; Principles, Challenges, and Open Questions</a> <br/>
    - <a href="https://arxiv.org/abs/1705.09406">Multimodal Machine Learning&#58; A Survey and Taxonomy</a> <br/>
    - <a href="https://arxiv.org/abs/1206.5538">Representation Learning&#58; A Review and New Perspectives</a> <br/>

- date: 1/27
  title: >
    Week 2: <strong>Dimensions of heterogeneity</strong> <a href="11877_week2.pdf">[synopsis]</a>
  topics:
    - What is a taxonomy of the dimensions in which modalities can be heterogeneous?
    - Heterogeneity is also often seen in several other ML subfields (e.g., domain adaptation, domain shift, transfer learning, multitask learning, federated learning, etc). What are some similarities and differences between the notions of heterogeneity between MMML and these fields? Can definitions or methods in each area be adapted to benefit other research areas?
    - How can we formalize these dimensions of heterogeneity, and subsequently estimate these measures to quantify the degree in which modalities are different?
    - Modality heterogeneity often implies the design of specialized models capturing the unique properties of each modality. What are some tradeoffs in modality-specific vs modality-general models?
    - What are other modeling considerations that ideally should be informed based on how heterogeneous the input modalities are?
    - What are some risks if we were to ignore modality or task heterogeneity? What if we are unable to estimate modality or task heterogeneity accurately?
  readings:
    - <a href="https://arxiv.org/abs/2104.13478">Geometric Deep Learning&#58; Grids, Groups, Graphs, Geodesics, and Gauges</a> <br/>
    - <a href="https://link.springer.com/article/10.1186/s40537-017-0089-0">A Survey on Heterogeneous Transfer Learning</a> <br/>
    - <a href="https://arxiv.org/abs/1804.08328">Taskonomy&#58; Disentangling Task Transfer Learning</a> <br/>
    - <a href="https://arxiv.org/abs/1905.07553">Which Tasks Should Be Learned Together in Multi-task Learning?</a> <br/>
    - <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9084352">Federated Learning&#58; Challenges, Methods, and Future Directions</a> <br/>
    - <a href="http://proceedings.mlr.press/v28/zhang13d.pdf">Domain Adaptation under Target and Conditional Shift</a> <br/>
    - <a href="https://arxiv.org/abs/1802.03916">Detecting and Correcting for Label Shift with Black Box Predictors</a> <br/>
    - <a href="https://arxiv.org/abs/2002.02923">Geometric Dataset Distances via Optimal Transport</a> <br/>
    - <a href="https://arxiv.org/abs/2203.01311">HighMMT&#58; Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning</a> <br/>
    - <a href="https://link.springer.com/content/pdf/10.1023/A:1007379606734.pdf">Multitask Learning</a> <br/>
    - <a href="https://link.springer.com/chapter/10.1007/978-3-540-45167-9_41">Exploiting Task Relatedness for Multiple Task Learning</a> <br/>
    
- date: 2/3
  title: >
    Week 3: <strong>Modality connections</strong> <a href="11877_week3.pdf">[synopsis]</a>
  topics:
    - What are the reasons why modalities can be connected with each other? Come up with a taxonomy of various dimensions. Think along both statistical, data-driven dimensions and semantic, hypothesis or knowledge driven dimensions. What are the pros and cons of either approach in understanding modality connections?
    - Are connections always strong and one-to-one? Reflect on what could make some cross-modal connections stronger or weaker, including many-to-many connections, ambiguity or noises?
    - Given trained multimodal models, how can we understand or visualize the nature of connections captured by the model?
    - What formalism or framework could be used to formalize cross-modal connections? How can we subsequently define estimators, where we can accurately quantify the presence of each type of connection given a dataset? How much knowledge of each modality do we need in order to estimate modality connections?
    - Linking back to week 1’s discussion on heterogeneity, how would you relate the concepts of heterogeneity and connections? How is heterogeneity affecting the study of crossmodal connections and inversely, how connections should be taken into consideration when heterogeneity is studied? Are connections also present in homogenous settings?
  readings:
    - <a href="https://arxiv.org/abs/2005.10243">What Makes for Good Views for Contrastive Learning?</a> <br/>
    - <a href="https://link.springer.com/article/10.1007/s13735-019-00187-6">Characterization and Classification of Semantic Image-text Relations</a> <br/>
    - <a href="https://www.emerald.com/insight/content/doi/10.1108/00220410310506303/full/pdf?title=a-taxonomy-of-relationships-between-images-and-text">A Taxonomy of Relationships Between Images and Text</a> <br/>
    - <a href="https://arxiv.org/abs/1511.06361">Order-Embeddings of Images and Language</a> <br/>
    - <a href="https://link.springer.com/content/pdf/10.1007/s10994-005-0913-1.pdf">Corpus-based Learning of Analogies and Semantic Relations</a> <br/>
    - <a href="https://distill.pub/2021/multimodal-neurons/">Multimodal Neurons in Artificial Neural Networks</a> <br/>
    - <a href="https://arxiv.org/abs/1607.07295">Learning Aligned Cross-Modal Representations from Weakly Aligned Data</a> <br/>
    - <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9363924">Toward Causal Representation Learning</a> <br/>
    - <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358050">A Review of Relational Machine Learning for Knowledge Graphs</a> <br/>
    - <a href="https://chinese.wooster.edu/files/barthes.pdf">Image-Music-Text</a> <br/>
    - <a href="https://arxiv.org/abs/1805.11222">Unsupervised Alignment of Embeddings with Wasserstein Procrustes</a> <br/>
    - <a href="https://arxiv.org/abs/1905.06922">On Variational Bounds of Mutual Information</a> <br/>
    - <a href="https://journals.sagepub.com/doi/epdf/10.1177/1470357205055928">A System for Image–text Relations in New (and Old) Media</a> <br/>
  
- date: 2/10
  title: >
    Week 4: <strong>Modality interactions</strong> <a href="11877_week4.pdf">[synopsis]</a>
  topics:
    - What are the different ways in which modalities can interact with each other when used for a prediction tasks? Think across both semantic and statistical perspectives. Can we formalize a taxonomy of such interactions, which will enable us to compare and contrast them more precisely? In fact, should we even try creating such a taxonomy?
    - Can you think of ways modalities could interact with each others, even if there is no prediction task? How are modalities interacting during cross-modal translation? During multimodal generation?
    - Linking back to last week’s discussion, are there cases where modalities are connected but do not interact? Or interact but are not connected? Can we design formal experiments to test either hypotheses?
    - What mathematical or empirical frameworks can be used to formalize the meaning of interactions? How can we subsequently define estimators, where we can accurately quantify the presence of each type of interactions given a dataset?
    - Some definitions (from the semantic category) typically require human interactions to detect and quantify interactions. What are some opportunities and limitations of using human judgment to analyze interactions? Can we potentially design estimators to automate the human labeling process?
    - What are the design decisions (aka inductive biases) that can be used when modeling each type of interaction in machine learning models?
    - What are the advantages and drawbacks of designing models to capture each type of cross-modal interaction? Consider not just prediction performance, but tradeoffs in time/space complexity, interpretability, etc.
  readings:
    - <a href="https://www.journals.uchicago.edu/doi/epdf/10.1086/431246">Issues in the Classification of Multimodal Communication Signals</a> <br/>
    - <a href="https://arxiv.org/abs/1004.2515">Nonnegative Decomposition of Multivariate Information</a> <br/>
    - <a href="https://www.sciencedirect.com/science/article/pii/S0378216608003056">Language and Image Interaction in Cartoons&#58; Towards a Multimodal Theory of Humor</a> <br/>
    - <a href="https://piazza.com/class_profile/get_resource/lcv0w9mqjzy1kx/lds3ewt4t4g6w4">Multimodality and Reading&#58; The Construction of Meaning Through Image-text Interaction</a> <br/>
    - <a href="https://dl.acm.org/doi/abs/10.1145/1228175.1228254">Examining the Redundancy of Multimodal Input</a> <br/>
    - <a href="https://aclanthology.org/2020.emnlp-main.62/">Does my Multimodal Model Learn Cross-modal Interactions? It’s harder to tell than you might think!</a> <br/>
    - <a href="https://arxiv.org/abs/2302.12247">Quantifying & Modeling Feature Interactions&#58; An Information Decomposition Framework</a> <br/>
    - <a href="https://arxiv.org/abs/2006.10965">How Does this Interaction Affect Me? Interpretable Attribution for Feature Interactions</a> <br/>
    - <a href="https://arxiv.org/abs/cs/0308002">Quantifying and Visualizing Attribute Interactions</a> <br/>
    - <a href="https://www.sesp.org/files/The%20Moderator-Baron.pdf">The Moderator-Mediator Variable Distinction in Social Psychological Research&#58; Conceptual, Strategic, and Statistical Considerations</a> <br/>

- date: 2/17
  title: >
    Week 5: <strong>Modality utility, tradeoffs, and selection</strong> <a href="11877_week5.pdf">[synopsis]</a>
  topics:
    - What are the different ways in which modalities can be useful for a task? How can we measure the utility of a modality for a task, given only access to the dataset (i.e., before designing and training a model)?
    - What are the criterion for which we should add or select modalities for a task? Is minimizing redundancy the only goal? Are there benefits in maximizing redundancy? Are there other criterion we should consider too?
    - Is modality selection the same as feature selection? What are the potential differences and new technical challenges in modality selection but not present in conventional feature selection? 
    - Given trained models, how can we estimate how important each modality was when making the prediction? How were these modalities used separately and in interaction with other modalities?
    - What are the different ways in which modalities can be harmful for a task? Think about a list of reasons why we would prefer to not use a modality. How can we quantify these potential risks?
    - What are some solutions for tackling these risks and biases in multimodal datasets and models? How can we properly identify, visualize and eventually reduce these risks in multimodal datasets and models?
    - Can we come up with guidelines that compare the tradeoffs between modality benefits and risks? How can we then integrate these insights into multimodal model design? Will integrating these insights help?
  readings:
    - <a href="https://proceedings.mlr.press/v180/cheng22a.html">Greedy Modality Selection via Approximate Submodular Maximization</a> <br/>
    - <a href="https://proceedings.mlr.press/v162/wu22d.html">Characterizing and Overcoming the Greedy Nature of Learning in Multi-modal Deep Neural Networks</a> <br/>
    - <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4082064">Information Value Theory</a> <br/>
    - <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1100705">A New Look at the Statistical Model Identification</a> <br/>
    - <a href="https://www.jmlr.org/papers/volume5/yu04a/yu04a.pdf">Efficient Feature Selection via Analysis of Relevance and Redundancy</a> <br/>
    - <a href="https://dl.acm.org/doi/pdf/10.1145/3462244.3479897">Bias and Fairness in Multimodal Machine Learning&#58; A Case Study of Automated Video Interviews</a> <br/>
    - <a href="https://arxiv.org/abs/2110.14375">Perceptual Score&#58; What Data Modalities Does Your Model Perceive?</a> <br/>
    - <a href="https://www.nature.com/articles/s42256-020-00257-z">Shortcut Learning in Deep Neural Networks</a> <br/>
    - <a href="https://arxiv.org/abs/2110.01963">Multimodal Datasets&#58; Misogyny, Pornography, and Malignant Stereotypes</a> <br/>
    - <a href="https://arxiv.org/abs/2107.07502">MultiBench&#58; Multiscale Benchmarks for Multimodal Representation Learning</a> <br/>
    - <a href="https://par.nsf.gov/servlets/purl/10090424#:~:text=Conditional%20entropy%20is%20an%20information,9%2C20%2C21%5D">Submodularity Issues in Value-of-information-based Sensor Placement</a> <br/>

- date: 2/24
  title: >
    Week 6: <strong>Pretraining and scaling</strong> <a href="11877_week6.pdf">[synopsis]</a>
  topics:
    - Is large-scale pretraining the way forward for building general AI models? What information potentially cannot be captured by pretraining? What are some potential risks of pretraining and scenarios where we should not use pretrained models?
    - How can we, in an academic environment, do impactful research in multimodal pretraining? What would be your proposed multi-year research agenda in this topic?
    - What are the types of cross-modal interactions that are likely to be modeled by current pretrained models? What cross-modal interactions will be harder to model with these methods? Do you have proposals for different pretraining data, architectures, or objectives that can better capture these interactions?
    - How can we best integrate multimodality into pretrained language models? What kind of additional data and modeling/optimization decisions do we need?
    - What are the different design decisions when integrating multimodal information in pretraining models and objectives? What are the main advantages and drawbacks of these design choices? Consider not just prediction performance, but tradeoffs in time/space complexity, interpretability, and so on.
    - How can we evaluate the type of multimodal information learned in pretrained models? One approach is to look at downstream tasks, but what are other ways to uncover the knowledge stored in pretrained models?  
  readings:
    - <a href="https://arxiv.org/abs/2301.03728">Scaling Laws for Generative Mixed-Modal Language Models</a> <br/>
    - <a href="https://arxiv.org/abs/2205.06175">A Generalist Agent</a> <br/>
    - <a href="https://arxiv.org/abs/2203.01311">HighMMT&#58; Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning</a> <br/>
    - <a href="https://arxiv.org/abs/2010.14701">Scaling Laws for Autoregressive Generative Modeling</a> <br/>
    - <a href="https://arxiv.org/abs/2301.12597">BLIP-2&#58; Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a> <br/>
    - <a href="https://arxiv.org/abs/2203.17247">VL-InterpreT&#58; An Interactive Visualization Tool for Interpreting Vision-Language Transformers</a> <br/>
    - <a href="https://arxiv.org/abs/2210.09263">Vision-Language Pre-training&#58; Basics, Recent Advances, and Future Trends</a> <br/>
    - <a href="https://arxiv.org/abs/2112.06825">VL-Adapter&#58; Parameter-Efficient Transfer Learning for Vision-and-Language Tasks</a> <br/>
    - <a href="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</a> <br/>
    - <a href="https://arxiv.org/abs/2011.15124">Multimodal Pretraining Unmasked&#58; A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs</a> <br/>
    - <a href="https://arxiv.org/abs/2108.07258">On the Opportunities and Risks of Foundation Models</a> <br/>
    - <a href="https://arxiv.org/abs/2204.14198">Flamingo&#58; a Visual Language Model for Few-Shot Learning</a> <br/>

- date: 3/3
  title: >
    Week 7: <strong>Multimodal reasoning</strong>
  topics:
    - What is reasoning, and what are its subchallenges? What could be a taxonomy of the main processes involved in reasoning? What are some potential formal definitions for reasoning?
    - Are there unique technical challenges that arise because reasoning is performed on multimodal versus unimodal data? How can we start studying these challenges in future research? Try to link it back to our previous definition of heterogeneity, connections, and interactions when thinking about multimodal reasoning challenges.
    - What are the main advantages of reasoning-based approaches, when compared to the large-scale pre-training methods discussed last week? What are the potential issues with reasoning-based methods? Can we come up with a research agenda that combines the best of both worlds?
    - Can we perform reasoning on very large datasets? Can pre-training methods eventually learn reasoning processes similar to humans? Or will we still need human and domain knowledge to some extent?
    - What are some ways to uncover the reasoning capabilities of multimodal models? What additional techniques do we need over measuring reasoning of unimodal models?
    - To what extent do we need external knowledge when performing reasoning, specifically multimodal reasoning? What type of external knowledge is likely to be needed to succeed in multimodal reasoning?
  readings:
    - <a href="https://arxiv.org/abs/2302.00923">Multimodal Chain-of-Thought Reasoning in Language Models</a> <br/>
    - <a href="https://arxiv.org/abs/1905.13211">What Can Neural Networks Reason About?</a> <br/>
    - <a href="https://www.amacad.org/publication/curious-case-commonsense-intelligence">The Curious Case of Commonsense Intelligence</a> <br/>
    - <a href="https://arxiv.org/abs/2212.10403">Towards Reasoning in Large Language Models&#58; A Survey</a> <br/>
    - <a href="https://arxiv.org/abs/2210.00312">Multimodal Analogical Reasoning over Knowledge Graphs</a> <br/>
    - <a href="https://arxiv.org/abs/2210.15037">Generalization Differences between End-to-End and Neuro-Symbolic Vision-Language Reasoning Systems</a> <br/>
    - <a href="https://arxiv.org/abs/2204.00598">Socratic Models&#58; Composing Zero-Shot Multimodal Reasoning with Language</a> <br/>
    - <a href="https://arxiv.org/abs/2204.03162">Winoground&#58; Probing Vision and Language Models for Visio-Linguistic Compositionality</a> <br/>
    - <a href="https://arxiv.org/abs/2209.09513">Learn to Explain&#58; Multimodal Reasoning via Thought Chains for Science Question Answering</a> <br/>
    - <a href="https://aclanthology.org/2020.acl-tutorials.7.pdf">Commonsense Reasoning for Natural Language Processing</a> <br/>
    - <a href="https://proceedings.mlr.press/v176/chang22a/chang22a.pdf">WebQA&#58; A Multimodal Multihop NeurIPS Challenge</a> <br/>
  
- date: 3/10
  title: >
    Week 8: <strong>No classes – Spring break</strong>
  topics:
    - None!
  readings:
    - None!

- date: 3/17
  title: >
    Week 9: <strong>Brain and multimodal perception</strong>
  topics:
  
  readings:
  
- date: 3/24
  title: >
    Week 10: <strong>Empirical and theoretical frameworks</strong>
  topics:
  
  readings:
  
- date: 3/31
  title: >
    Week 11: <strong>Quantification and visualization</strong>
  topics:
  
  readings:

- date: 4/7
  title: >
    Week 12: <strong>No classes – CMU Carnival</strong>
  topics:
    - None!
  readings:
    - None!
  
- date: 4/14
  title: >
    Week 13: <strong>Generalization and optimization</strong>
  topics:
  
  readings:
  
- date: 4/21
  title: >
    Week 14: <strong>Open research questions</strong>
  topics:
  
  readings:
  
- date: 4/28
  title: >
    Week 15: <strong>Project presentations</strong>
  topics:
  
  readings:
